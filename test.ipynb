{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Haozhou Xu\n",
    "# PID: A69032157\n",
    "# Date: 2024.10.25\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_size, head_num):\n",
    "        super(Attention, self).__init__()\n",
    "        assert embed_size % head_num == 0\n",
    "        self.embed_size = embed_size\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = embed_size // head_num\n",
    "\n",
    "        # Separate Q, K, V layers for each attention head\n",
    "        self.q = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.k = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.v = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.fc = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        N, seq_len, num_features = x.shape\n",
    "\n",
    "        # Split the embedding into heads\n",
    "        q = self.q(x).reshape(N, seq_len, self.head_num, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = self.k(x).reshape(N, seq_len, self.head_num, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = self.v(x).reshape(N, seq_len, self.head_num, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Calculate\n",
    "        score = torch.matmul(q, k.permute(0, 1, 3, 2)) / np.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # (batch_size, 1, seq_len, seq_len)\n",
    "            score = score.masked_fill(mask == 0, float('-inf'))\n",
    "        weight = F.softmax(score, dim=-1)\n",
    "        # Perhaps we can add dropout here\n",
    "        # weight = self.dropout(weight)\n",
    "        out = torch.matmul(weight, v).permute(0, 2, 1, 3).reshape(N, seq_len, self.embed_size)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0):\n",
    "        super(FNN, self).__init__()\n",
    "        assert num_layers >= 1\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ) for i in range(num_layers - 1)])\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, head_num, hidden_size, num_layers=1, dropout=0):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        assert num_layers >= 1\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Sequential(\n",
    "                Attention(embed_size, head_num),\n",
    "                FNN(embed_size, hidden_size, embed_size, num_layers=1, dropout=dropout)\n",
    "            ) for i in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer[0](x, mask) + x\n",
    "            x = layer[1](x) + x\n",
    "        return self.norm(x)\n",
    "    \n",
    "    def encode(self, x, mask=None):\n",
    "        x = self.forward(x, mask)\n",
    "        x = x.mean(dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据形状: torch.Size([3, 10, 32])\n",
      "TransformerEncoder 输出形状: torch.Size([3, 10, 32])\n",
      "Encoded 输出形状: torch.Size([3, 32])\n"
     ]
    }
   ],
   "source": [
    "# 测试TransformerEncoder类\n",
    "embed_size = 32\n",
    "head_num = 4\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "seq_len = 10\n",
    "batch_size = 3\n",
    "\n",
    "# 模拟输入数据和mask\n",
    "input_data = torch.randn(batch_size, seq_len, embed_size)\n",
    "print(\"输入数据形状:\", input_data.shape)  # 期望形状: (batch_size, seq_len, embed_size)\n",
    "mask = torch.ones(batch_size, seq_len, seq_len)  # 可选的mask\n",
    "\n",
    "# 初始化模型并测试\n",
    "model = TransformerEncoder(embed_size, head_num, hidden_size, num_layers=num_layers, dropout=0.1)\n",
    "output = model(input_data, mask)\n",
    "print(\"TransformerEncoder 输出形状:\", output.shape)  # 期望形状: (batch_size, seq_len, embed_size)\n",
    "encoded_output = model.encode(input_data, mask)\n",
    "print(\"Encoded 输出形状:\", encoded_output.shape)  # 期望形状: (batch_size, embed_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_heads, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Positional Encoding Layer\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, 32, embed_dim))  # 32 is block_size from main.py\n",
    "        \n",
    "        # Transformer Encoder Layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=4*embed_dim,  # 标准transformer使用4倍的embed_dim\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Classifier layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate attention mask (1 for non-padding, 0 for padding)\n",
    "        mask = (x != 0).float()\n",
    "        \n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]\n",
    "        \n",
    "        # Create attention mask for transformer\n",
    "        # Convert boolean mask to float and then to bool\n",
    "        attention_mask = (mask == 0).bool()\n",
    "        \n",
    "        # Transformer layers\n",
    "        x = self.transformer(x, src_key_padding_mask=attention_mask)\n",
    "        \n",
    "        # Mean pooling over sequence length\n",
    "        x = (x * mask.unsqueeze(-1)).sum(dim=1) / mask.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # Classification layers\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and creating tokenizer ...\n",
      "Vocabulary size is 5755\n",
      "Epoch 1/15, Loss: 1.0939, Accuracy: 37.91%\n",
      "Epoch 2/15, Loss: 1.0809, Accuracy: 43.31%\n",
      "Epoch 3/15, Loss: 1.0757, Accuracy: 44.55%\n",
      "Epoch 4/15, Loss: 1.0736, Accuracy: 44.79%\n",
      "Epoch 5/15, Loss: 1.0714, Accuracy: 44.55%\n",
      "Epoch 6/15, Loss: 1.0682, Accuracy: 44.74%\n",
      "Epoch 7/15, Loss: 1.0702, Accuracy: 44.65%\n",
      "Epoch 8/15, Loss: 1.0690, Accuracy: 44.55%\n",
      "Epoch 9/15, Loss: 1.0682, Accuracy: 44.60%\n",
      "Epoch 10/15, Loss: 1.0691, Accuracy: 44.74%\n",
      "Epoch 11/15, Loss: 1.0667, Accuracy: 44.74%\n",
      "Epoch 12/15, Loss: 1.0654, Accuracy: 44.79%\n",
      "Epoch 13/15, Loss: 1.0663, Accuracy: 44.84%\n",
      "Epoch 14/15, Loss: 1.0635, Accuracy: 44.84%\n",
      "Epoch 15/15, Loss: 1.0632, Accuracy: 44.74%\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 157\u001b[0m\n\u001b[1;32m    155\u001b[0m test_CLS_dataset \u001b[38;5;241m=\u001b[39m SpeechesClassificationDataset(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeechesdataset/test_CLS.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m test_CLS_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_CLS_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, collate_fn\u001b[38;5;241m=\u001b[39mcollate_batch)\n\u001b[0;32m--> 157\u001b[0m \u001b[43mcompute_classifier_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_CLS_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# for the language modeling task, you will iterate over the training data for a fixed number of iterations like this:\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (xb, yb) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_LM_loader):\n",
      "Cell \u001b[0;32mIn[22], line 76\u001b[0m, in \u001b[0;36mcompute_classifier_accuracy\u001b[0;34m(classifier, data_loader)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, Y \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     75\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), Y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 76\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     78\u001b[0m     total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m Y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/analysis/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 52\u001b[0m, in \u001b[0;36mFNNClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m (mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Transformer layers\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Mean pooling over sequence length\u001b[39;00m\n\u001b[1;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m*\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/analysis/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/analysis/lib/python3.8/site-packages/torch/nn/modules/transformer.py:258\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m src_key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     why_not_sparsity_fast_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc_key_padding_mask was None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (((\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_check\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_check)\n\u001b[0;32m--> 258\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nested_tensor_from_mask_left_aligned\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_not\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    259\u001b[0m     why_not_sparsity_fast_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_check enabled, and src and src_key_padding_mask was not left aligned\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mis_nested:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "\n",
    "from tokenizer import SimpleTokenizer\n",
    "from dataset import SpeechesClassificationDataset, LanguageModelingDataset\n",
    "\n",
    "seed = 42\n",
    "PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\"\"\" Hyperparameters to use for training to roughly match \n",
    "the numbers mentioned in the assignment description \"\"\"\n",
    "batch_size = 16  # Number of independent sequences  we will process in parallel\n",
    "block_size = 32  # Maximum context length for predictions\n",
    "learning_rate = 1e-3  # Learning rate for the optimizer\n",
    "n_embd = 64  # Embedding dimension\n",
    "n_head = 2  # Number of attention heads\n",
    "n_layer = 4  # Number of transformer layers\n",
    "\n",
    "\n",
    "eval_interval = 100  # How often to evaluate train and test perplexity during training\n",
    "max_iters = 500 # For language modeling, we can process all the batches for the entire dataset, but that takes a while, so we'll limit it to 500 iterations. For batch size of 16 and block size of  32, this is roughly, this is  500 * 16 * 32 = 256000 tokens, SOTA LMs are trained on trillions of tokens, so this is a very small dataset.\n",
    "eval_iters = 200  # Number of iterations to evaluate perplexity on the test set\n",
    "\n",
    "\n",
    "## classifier training hyperparameters. It is a simple 1 hidden layer feedforward network, with input \n",
    "## size of 64, hidden size of 50 and output size of 3.\n",
    "\n",
    "n_input = 64  # Input size for the classifier, should match the embedding size of the transformer\n",
    "n_hidden = 100  # Hidden size for the classifier\n",
    "n_output = 3  # Output size for the classifier, we have 3 classes\n",
    "epochs_CLS = 15 # epochs for classifier training\n",
    "\n",
    "def load_texts(directory):\n",
    "    \"\"\"\n",
    "    This function loads all texts from the specified directory, ignoring any files with \"test\" in their name. The text is used for \"training\" the tokenizer. Since our tokenizer is simple, we don't need to do any training, but we still need to ignore the test data. \n",
    "    \"\"\"\n",
    "\n",
    "    texts = []\n",
    "    files = os.listdir(directory)\n",
    "    for filename in files: \n",
    "        if \"test\" in filename:  ## don't \"read test files\"\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\" Collate a batch of data into a single tensor with padding.\"\"\"\n",
    "    data, labels = zip(*batch)  # Separate the data and labels\n",
    "    # Pad sequences to the fixed length\n",
    "    padded_sequences = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    padded_sequences = padded_sequences[:, :block_size]  # Truncate if longer\n",
    "    # Add padding if shorter\n",
    "    padded_sequences = torch.nn.functional.pad(padded_sequences, (0, max(0, block_size - padded_sequences.shape[1])), \"constant\", 0)\n",
    "    labels = torch.stack(labels)  \n",
    "    return padded_sequences, labels\n",
    "\n",
    "def compute_classifier_accuracy(classifier, data_loader):\n",
    "    \"\"\" Compute the accuracy of the classifier on the data in data_loader.\"\"\"\n",
    "    classifier.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for X, Y in data_loader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            outputs = classifier(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == Y).sum().item()\n",
    "            total_samples += Y.size(0)\n",
    "        accuracy = (100 * total_correct / total_samples)\n",
    "        classifier.train()\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "def compute_perplexity(decoderLMmodel, data_loader, eval_iters=100):\n",
    "    \"\"\" Compute the perplexity of the decoderLMmodel on the data in data_loader.\n",
    "    Make sure to use the cross entropy loss for the decoderLMmodel.\n",
    "    \"\"\"\n",
    "    decoderLMmodel.eval()\n",
    "    losses= []\n",
    "    for X, Y in data_loader:\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        loss = decoderLMmodel(X, Y) # your model should be computing the cross entropy loss\n",
    "        losses.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        if len(losses) >= eval_iters: break\n",
    "\n",
    "\n",
    "    losses = torch.tensor(losses)\n",
    "    mean_loss = losses.mean()\n",
    "    perplexity = torch.exp(mean_loss).item()  # Calculate perplexity as exp(mean loss)\n",
    "\n",
    "    decoderLMmodel.train()\n",
    "    return perplexity\n",
    "\n",
    "def train_CLS_model(tokenizer, train_CLS_loader, epochs_CLS):\n",
    "    cls = FNNClassifier(tokenizer.vocab_size, n_embd, n_hidden, n_output, n_head, n_layer, dropout=0.1).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(cls.parameters(), lr=0.001)\n",
    "    cls.to(device)\n",
    "    \n",
    "    for epoch in range(epochs_CLS):\n",
    "        epoch_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for xb, yb in train_CLS_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = cls(xb)\n",
    "            loss = loss_fn(outputs, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate the total loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            total_correct += (outputs.argmax(1) == yb).sum().item()\n",
    "            total_samples += yb.size(0)\n",
    "        \n",
    "        # Sum of the loss for the entire epoch\n",
    "        avg_loss = epoch_loss / len(train_CLS_loader)\n",
    "        accuracy = (total_correct / total_samples) * 100\n",
    "        print(f\"Epoch {epoch + 1}/{epochs_CLS}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return cls\n",
    "\n",
    "print(\"Loading data and creating tokenizer ...\")\n",
    "texts = load_texts('speechesdataset')\n",
    "tokenizer = SimpleTokenizer(' '.join(texts)) # create a tokenizer from the data\n",
    "print(\"Vocabulary size is\", tokenizer.vocab_size)\n",
    "\n",
    "train_CLS_dataset = SpeechesClassificationDataset(tokenizer, \"speechesdataset/train_CLS.tsv\")\n",
    "train_CLS_loader = DataLoader(train_CLS_dataset, batch_size=batch_size,collate_fn=collate_batch,shuffle=True)\n",
    "\n",
    "\n",
    "inputfile = \"speechesdataset/train_LM.txt\"\n",
    "with open(inputfile, 'r', encoding='utf-8') as f:\n",
    "    lmtrainText = f.read()\n",
    "train_LM_dataset = LanguageModelingDataset(tokenizer, lmtrainText,  block_size)\n",
    "train_LM_loader = DataLoader(train_LM_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# for the classification task, you will train for a fixed number of epochs like this:\n",
    "# CLS training code here\n",
    "cls = train_CLS_model(tokenizer, train_CLS_loader, epochs_CLS)\n",
    "test_CLS_dataset = SpeechesClassificationDataset(tokenizer, \"speechesdataset/test_CLS.tsv\")\n",
    "test_CLS_loader = DataLoader(test_CLS_dataset, batch_size=batch_size, collate_fn=collate_batch)\n",
    "compute_classifier_accuracy(cls, test_CLS_loader)\n",
    "# for the language modeling task, you will iterate over the training data for a fixed number of iterations like this:\n",
    "for i, (xb, yb) in enumerate(train_LM_loader):\n",
    "    if i >= max_iters:\n",
    "        break\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    # LM training code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_heads, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Positional Encoding Layer\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, 32, embed_dim))\n",
    "        \n",
    "        # Transformer Encoder Layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,  # 使用指定的hidden_dim\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation='gelu'  # 使用GELU激活函数\n",
    "        )\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Classifier layers - 使用更深的分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 初始化权重\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Generate attention mask (1 for non-padding, 0 for padding)\n",
    "        mask = (x != 0).float()\n",
    "        \n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]\n",
    "        \n",
    "        # Create attention mask for transformer\n",
    "        attention_mask = (mask == 0).bool()\n",
    "        \n",
    "        # Transformer layers\n",
    "        x = self.transformer(x, src_key_padding_mask=attention_mask)\n",
    "        \n",
    "        # 使用 [CLS] token 或序列平均值\n",
    "        # 这里我们使用加权平均，给予较短序列更高的权重\n",
    "        weights = mask / (mask.sum(dim=1, keepdim=True) + 1e-8)\n",
    "        x = (x * weights.unsqueeze(-1)).sum(dim=1)\n",
    "        \n",
    "        # Classification layers\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and creating tokenizer ...\n",
      "Vocabulary size is 5755\n",
      "Epoch 1/15, Loss: 1.3763, Accuracy: 42.40%\n",
      "Epoch 2/15, Loss: 1.1286, Accuracy: 60.28%\n",
      "Epoch 3/15, Loss: 0.9233, Accuracy: 74.19%\n",
      "Epoch 4/15, Loss: 0.7257, Accuracy: 85.23%\n",
      "Epoch 5/15, Loss: 0.5805, Accuracy: 90.06%\n",
      "Epoch 6/15, Loss: 0.4900, Accuracy: 93.98%\n",
      "Epoch 7/15, Loss: 0.4259, Accuracy: 95.70%\n",
      "Epoch 8/15, Loss: 0.4139, Accuracy: 96.03%\n",
      "Epoch 9/15, Loss: 0.4110, Accuracy: 95.70%\n",
      "Epoch 10/15, Loss: 0.3757, Accuracy: 96.80%\n",
      "Epoch 11/15, Loss: 0.3594, Accuracy: 97.04%\n",
      "Epoch 12/15, Loss: 0.3306, Accuracy: 97.32%\n",
      "Epoch 13/15, Loss: 0.3248, Accuracy: 97.42%\n",
      "Epoch 14/15, Loss: 0.3231, Accuracy: 97.42%\n",
      "Epoch 15/15, Loss: 0.2900, Accuracy: 98.04%\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 187\u001b[0m\n\u001b[1;32m    185\u001b[0m test_CLS_dataset \u001b[38;5;241m=\u001b[39m SpeechesClassificationDataset(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeechesdataset/test_CLS.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m test_CLS_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_CLS_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, collate_fn\u001b[38;5;241m=\u001b[39mcollate_batch)\n\u001b[0;32m--> 187\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_classifier_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_CLS_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy on test set:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# for the language modeling task, you will iterate over the training data for a fixed number of iterations like this:\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 76\u001b[0m, in \u001b[0;36mcompute_classifier_accuracy\u001b[0;34m(classifier, data_loader)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, Y \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     75\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), Y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 76\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     78\u001b[0m     total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m Y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/analysis/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[29], line 63\u001b[0m, in \u001b[0;36mFNNClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m (mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Transformer layers\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# 使用 [CLS] token 或序列平均值\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 这里我们使用加权平均，给予较短序列更高的权重\u001b[39;00m\n\u001b[1;32m     67\u001b[0m weights \u001b[38;5;241m=\u001b[39m mask \u001b[38;5;241m/\u001b[39m (mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/analysis/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/analysis/lib/python3.8/site-packages/torch/nn/modules/transformer.py:258\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m src_key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     why_not_sparsity_fast_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc_key_padding_mask was None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (((\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_check\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_check)\n\u001b[0;32m--> 258\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nested_tensor_from_mask_left_aligned\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_not\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    259\u001b[0m     why_not_sparsity_fast_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_check enabled, and src and src_key_padding_mask was not left aligned\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mis_nested:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "\n",
    "from tokenizer import SimpleTokenizer\n",
    "from dataset import SpeechesClassificationDataset, LanguageModelingDataset\n",
    "\n",
    "seed = 42\n",
    "PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\"\"\" Hyperparameters to use for training to roughly match \n",
    "the numbers mentioned in the assignment description \"\"\"\n",
    "batch_size = 16  # Number of independent sequences  we will process in parallel\n",
    "block_size = 32  # Maximum context length for predictions\n",
    "learning_rate = 1e-3  # Learning rate for the optimizer\n",
    "n_embd = 64  # Embedding dimension\n",
    "n_head = 2  # Number of attention heads\n",
    "n_layer = 4  # Number of transformer layers\n",
    "\n",
    "\n",
    "eval_interval = 100  # How often to evaluate train and test perplexity during training\n",
    "max_iters = 500 # For language modeling, we can process all the batches for the entire dataset, but that takes a while, so we'll limit it to 500 iterations. For batch size of 16 and block size of  32, this is roughly, this is  500 * 16 * 32 = 256000 tokens, SOTA LMs are trained on trillions of tokens, so this is a very small dataset.\n",
    "eval_iters = 200  # Number of iterations to evaluate perplexity on the test set\n",
    "\n",
    "\n",
    "## classifier training hyperparameters. It is a simple 1 hidden layer feedforward network, with input \n",
    "## size of 64, hidden size of 50 and output size of 3.\n",
    "\n",
    "n_input = 64  # Input size for the classifier, should match the embedding size of the transformer\n",
    "n_hidden = 100  # Hidden size for the classifier\n",
    "n_output = 3  # Output size for the classifier, we have 3 classes\n",
    "epochs_CLS = 15 # epochs for classifier training\n",
    "\n",
    "def load_texts(directory):\n",
    "    \"\"\"\n",
    "    This function loads all texts from the specified directory, ignoring any files with \"test\" in their name. The text is used for \"training\" the tokenizer. Since our tokenizer is simple, we don't need to do any training, but we still need to ignore the test data. \n",
    "    \"\"\"\n",
    "\n",
    "    texts = []\n",
    "    files = os.listdir(directory)\n",
    "    for filename in files: \n",
    "        if \"test\" in filename:  ## don't \"read test files\"\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\" Collate a batch of data into a single tensor with padding.\"\"\"\n",
    "    data, labels = zip(*batch)  # Separate the data and labels\n",
    "    # Pad sequences to the fixed length\n",
    "    padded_sequences = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    padded_sequences = padded_sequences[:, :block_size]  # Truncate if longer\n",
    "    # Add padding if shorter\n",
    "    padded_sequences = torch.nn.functional.pad(padded_sequences, (0, max(0, block_size - padded_sequences.shape[1])), \"constant\", 0)\n",
    "    labels = torch.stack(labels)  \n",
    "    return padded_sequences, labels\n",
    "\n",
    "def compute_classifier_accuracy(classifier, data_loader):\n",
    "    \"\"\" Compute the accuracy of the classifier on the data in data_loader.\"\"\"\n",
    "    classifier.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for X, Y in data_loader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            outputs = classifier(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == Y).sum().item()\n",
    "            total_samples += Y.size(0)\n",
    "        accuracy = (100 * total_correct / total_samples)\n",
    "        classifier.train()\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "def compute_perplexity(decoderLMmodel, data_loader, eval_iters=100):\n",
    "    \"\"\" Compute the perplexity of the decoderLMmodel on the data in data_loader.\n",
    "    Make sure to use the cross entropy loss for the decoderLMmodel.\n",
    "    \"\"\"\n",
    "    decoderLMmodel.eval()\n",
    "    losses= []\n",
    "    for X, Y in data_loader:\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        loss = decoderLMmodel(X, Y) # your model should be computing the cross entropy loss\n",
    "        losses.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        if len(losses) >= eval_iters: break\n",
    "\n",
    "\n",
    "    losses = torch.tensor(losses)\n",
    "    mean_loss = losses.mean()\n",
    "    perplexity = torch.exp(mean_loss).item()  # Calculate perplexity as exp(mean loss)\n",
    "\n",
    "    decoderLMmodel.train()\n",
    "    return perplexity\n",
    "\n",
    "def train_CLS_model(tokenizer, train_CLS_loader, epochs_CLS):\n",
    "    cls = FNNClassifier(tokenizer.vocab_size, n_embd, n_hidden, n_output, n_head, n_layer, dropout=0.1).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 使用Adam优化器，并添加学习率调度器\n",
    "    optimizer = torch.optim.Adam(cls.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', \n",
    "                                                         factor=0.5, patience=2, \n",
    "                                                         verbose=True)\n",
    "    \n",
    "    cls.to(device)\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for epoch in range(epochs_CLS):\n",
    "        cls.train()\n",
    "        epoch_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for xb, yb in train_CLS_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = cls(xb)\n",
    "            loss = loss_fn(outputs, yb)\n",
    "            \n",
    "            # 添加L2正则化\n",
    "            l2_lambda = 0.001\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "            for param in cls.parameters():\n",
    "                l2_reg += torch.norm(param)\n",
    "            loss += l2_lambda * l2_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(cls.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            total_correct += (outputs.argmax(1) == yb).sum().item()\n",
    "            total_samples += yb.size(0)\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_CLS_loader)\n",
    "        accuracy = (total_correct / total_samples) * 100\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs_CLS}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(accuracy)\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(cls.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    # 加载最佳模型\n",
    "    cls.load_state_dict(torch.load('best_model.pth'))\n",
    "    return cls\n",
    "\n",
    "print(\"Loading data and creating tokenizer ...\")\n",
    "texts = load_texts('speechesdataset')\n",
    "tokenizer = SimpleTokenizer(' '.join(texts)) # create a tokenizer from the data\n",
    "print(\"Vocabulary size is\", tokenizer.vocab_size)\n",
    "\n",
    "train_CLS_dataset = SpeechesClassificationDataset(tokenizer, \"speechesdataset/train_CLS.tsv\")\n",
    "train_CLS_loader = DataLoader(train_CLS_dataset, batch_size=batch_size,collate_fn=collate_batch,shuffle=True)\n",
    "\n",
    "\n",
    "inputfile = \"speechesdataset/train_LM.txt\"\n",
    "with open(inputfile, 'r', encoding='utf-8') as f:\n",
    "    lmtrainText = f.read()\n",
    "train_LM_dataset = LanguageModelingDataset(tokenizer, lmtrainText,  block_size)\n",
    "train_LM_loader = DataLoader(train_LM_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# for the classification task, you will train for a fixed number of epochs like this:\n",
    "# CLS training code here\n",
    "cls = train_CLS_model(tokenizer, train_CLS_loader, epochs_CLS)\n",
    "test_CLS_dataset = SpeechesClassificationDataset(tokenizer, \"speechesdataset/test_CLS.tsv\")\n",
    "test_CLS_loader = DataLoader(test_CLS_dataset, batch_size=batch_size, collate_fn=collate_batch)\n",
    "test_acc = compute_classifier_accuracy(cls, test_CLS_loader)\n",
    "print(\"Accuracy on test set:\", test_acc)\n",
    "# for the language modeling task, you will iterate over the training data for a fixed number of iterations like this:\n",
    "for i, (xb, yb) in enumerate(train_LM_loader):\n",
    "    if i >= max_iters:\n",
    "        break\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    # LM training code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
